{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading project data.\n",
      "traffic-signs-data.zip already downloaded.\n",
      "Data already unzipped.\n",
      "Unpickling file train.p.\n",
      "Unpickling file test.p.\n",
      "Returning ProjectData(train, test).\n",
      "Loading pre-preprocessed greyscale data...\n",
      "Unpickling file train_greyscale_preprocessed.p.\n",
      "Loading pre-preprocessed data...\n",
      "Unpickling file train_jiggered_preprocessed.p.\n",
      "Unpickling file valid_preprocessed.p.\n",
      "Unpickling file test_preprocessed.p.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "from reusable import file_loader\n",
    "from reusable import load\n",
    "from reusable import preprocessor\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "DISPLAY = False\n",
    "AWS = False\n",
    "\n",
    "orig_data = load.load_project_data()\n",
    "greyscale_data = preprocessor.load_greyscale_train_data()\n",
    "data = preprocessor.load_preprocessed_jiggered_data()\n",
    "\n",
    "orig_X_train, orig_y_train = (orig_data.train.features, orig_data.train.labels)\n",
    "orig_X_valid, orig_y_valid = (orig_data.valid.features, orig_data.valid.labels)\n",
    "orig_X_test, orig_y_test = (orig_data.test.features, orig_data.test.labels)\n",
    "greyscale_X, greyscale_y = (greyscale_data.features, greyscale_data.labels)\n",
    "X_train, y_train = (data.train.features, data.train.labels)\n",
    "X_valid, y_valid = (data.valid.features, data.valid.labels)\n",
    "X_test, y_test = (data.test.features, data.test.labels)\n",
    "\n",
    "if AWS is False:\n",
    "  X_train = X_train[0:10000]\n",
    "  y_train = y_train[0:10000]\n",
    "  \n",
    "def load_csv_to_dict(csv_file):\n",
    "  with open(csv_file, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    return {rows[0]:rows[1] for rows in reader}\n",
    "  \n",
    "sign_dict = load_csv_to_dict(\"../signnames.csv\")\n",
    "\n",
    "unique, counts = np.unique(orig_y_train, return_counts=True)\n",
    "unique_grey = np.unique(greyscale_y)\n",
    "# dict of original images\n",
    "train_dict = {u: [] for u in unique}\n",
    "grey_dict = {u: [] for u in unique}\n",
    "\n",
    "for i in range(len(orig_X_train)):\n",
    "  train_dict[orig_y_train[i]].append(orig_X_train[i])\n",
    "  grey_dict[greyscale_y[i]].append(greyscale_X[i])\n",
    "  \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of training examples = 29406\n",
      "Original number of testing examples = 29406\n",
      "Original image data shape = (32, 32, 3)\n",
      "Original number of classes = 43\n",
      "Original min count = 145\n",
      "Original max count = 1714\n",
      "\n",
      "Jiggered number of training examples = 10000\n",
      "Jiggered number of testing examples = 10000\n",
      "Jiggered image data shape = (32, 32, 1)\n",
      "Jiggered number of classes = 43\n",
      "Jiggered min count = 186\n",
      "Jiggered max count = 277\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "def data_info(features_train, labels_train, feautures_test, labels_test, name):\n",
    "  # Number of training examples\n",
    "  n_train = len(features_train)\n",
    "  # Number of testing examples.\n",
    "  n_test = len(labels_train)\n",
    "  # What's the shape of an traffic sign image?\n",
    "  image_shape = features_train[0].shape\n",
    "  # How many unique classes/labels there are in the dataset.\n",
    "  n_classes = len(set(y_train))\n",
    "  \n",
    "  indicies, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "  print(name, \"number of training examples =\", n_train)\n",
    "  print(name, \"number of testing examples =\", n_test)\n",
    "  print(name, \"image data shape =\", image_shape)\n",
    "  print(name, \"number of classes =\", n_classes)\n",
    "  print(name, \"min count =\", min(counts))\n",
    "  print(name, \"max count =\", max(counts))\n",
    "  \n",
    "data_info(orig_X_train, orig_y_train, orig_X_test, orig_y_test, \"Original\")\n",
    "print()\n",
    "data_info(X_train, y_train, X_test, y_test, \"Jiggered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc.\n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only run this for display.\n"
     ]
    }
   ],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def display_images(images, image_name, num_images, greyscale=False):\n",
    "  print(\"Class\", \"`\" + image_name + \"`\", \":\", len(images), \"samples.\")\n",
    "  fig = plt.figure(figsize = (6, 1))\n",
    "  fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "  for i in range(num_images):\n",
    "    axis = fig.add_subplot(1, num_images, i + 1, xticks=[], yticks=[])\n",
    "    rand_index = random.randint(0, len(images[i]-1))\n",
    "    if greyscale:\n",
    "      axis.imshow(images[rand_index].squeeze(), cmap='gray')\n",
    "    else:\n",
    "      axis.imshow(images[rand_index].squeeze())\n",
    "  plt.show()\n",
    "  print(\"--------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "if DISPLAY:\n",
    "  for k in train_dict.keys():\n",
    "    display_images(train_dict[k], sign_dict[str(k)], 10)\n",
    "    display_images(grey_dict[k], sign_dict[str(k)] + \" greyscale\", 10, greyscale=True)\n",
    "else:\n",
    "  print(\"Only run this for display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Count number of each sign in test and train datasets\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def hist(data, name):\n",
    "  indicies, counts = np.unique(data, return_counts=True)\n",
    "  # the histogram of the data\n",
    "  n, bins, patches = plt.hist(data, len(indicies), facecolor='green')\n",
    "\n",
    "  plt.xlabel('Sign index')\n",
    "  plt.ylabel('Count')\n",
    "  plt.title('Count  of dataset ' + name)\n",
    "  plt.axis([0, len(indicies), 0, max(counts)])\n",
    "  plt.grid(True)\n",
    "\n",
    "  plt.show()\n",
    "  \n",
    "def min_counts(data, num_to_print=5):\n",
    "  indicies, counts = np.unique(data, return_counts=True)\n",
    "  arr = sorted(list(zip(indicies, counts)), key=lambda x: x[1])\n",
    "  print([(sign_dict[str(sign[0])], sign[1]) for sign in arr[0:num_to_print]])\n",
    "\n",
    "def max_counts(data, num_to_print=5):\n",
    "  indicies, counts = np.unique(data, return_counts=True)\n",
    "  arr = sorted(list(zip(indicies, counts)), key=lambda x: x[1], reverse=True)\n",
    "  print([(sign_dict[str(sign[0])], sign[1]) for sign in arr[0:num_to_print]])\n",
    "  \n",
    "if DISPLAY:\n",
    "  min_counts(orig_y_train)\n",
    "  max_counts(orig_y_train)\n",
    "  hist(orig_y_train, 'train')\n",
    "\n",
    "  min_counts(y_test)\n",
    "  max_counts(y_test)\n",
    "  hist(y_test, 'test')\n",
    "\n",
    "  min_counts(y_train)\n",
    "  max_counts(y_train)\n",
    "  hist(y_train, 'jiggered train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "#### Preprocess Steps\n",
    "\n",
    "I have done all my preprocessing in helper files. Preprocessing was taking a long time so I found it easier to save the files as pickles than continue to reload them. My processing steps are:\n",
    "\n",
    "1. Convert to greyscale\n",
    "2. Scale between -1 and 1\n",
    "3. Split out validation data\n",
    "\n",
    "I converted the images to greyscale because Images had varying levels of brightness and contrast. COnverting to greyscale imporoved the contrast dramatically. I scaled the images between -1 and 1 because it is easier to run the optimization when the data is normally distribuated about 0 with a standard deviation of 1. I decided against using the data points in the csv file to crop the images. In looking at the images, I felt like he images had already been cropped sufficently for a Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. **Optional**: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "When looking at the graphs of the data above, I noticed there were a lot less (10X) of some images than others. I am not sure if this data is reflective of the signs on German roads or not. If it is then it may make sense to actually leave the data as is. It is possible that a data bias will result in the model \"guessing\" the a more popular sign when it is unsure. Biasing the model toward the most popular sign may not be the best idea. Anther way to bias the model could be by having the most signs for the most important information (Where getting the sign wrong could be the most dangerous, for example). In the end I wasn't sure what if any bias I should/could apply based on the data so I instead decided that there should be equal numbers of all signs. I first did this by limiting the dataset to the smallest number of on individual sign. This made it super fast to train the model but resulted in a low validation accuracy. After reading the Yann Lecunn paper linked above, I realized I could \"jigger\" the images to create more of them. In general the model shoudl be robuse to different lighting conditions and different angles and different positions in the image. With this in mind, I decided to add additional data points so all signs had the same number of data points. I did this by applying random shifts, rotations and gamma values in a range of values. The range was based on the values used in the LeCunn paper and an eye test. I ended up using the maximum individual sign count as the number of signs but I could also add even more data points by increasing this number and jiggering all sign types. \n",
    "\n",
    "Looking at the images, it is hard to tell what is a \"jiggered\" image and what isn't so I think the jiggered data is within the relm of reason. \n",
    "\n",
    "- Original number of training examples = 39209\n",
    "- Original number of testing examples = 39209\n",
    "- Original image data shape = (32, 32, 3)\n",
    "- Original number of classes = 43\n",
    "- Original min count = 210\n",
    "- Original max count = 2250\n",
    "\n",
    "\n",
    "- Jiggered number of training examples = 96750\n",
    "- Jiggered number of testing examples = 96750\n",
    "- Jiggered image data shape = (32, 32, 1)\n",
    "- Jiggered number of classes = 43\n",
    "- Jiggered min count = 2250\n",
    "- Jiggered max count = 2250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "This model is \n",
    "* CNN(stride=1, kernal=5, depth=16)\n",
    "* IncpetionModule(depth=32)\n",
    "* InceptionModule(depth=64)\n",
    "* Dropout(0.7)\n",
    "* Flattened()\n",
    "* FullyConnected(depth=64)\n",
    "* Dropout(0.6)\n",
    "* FullyConnected(depth=128)\n",
    "* Dropout(0.5)\n",
    "* Out (depth=10)\n",
    "\n",
    "Based on this [article](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/.An), an  inception module is:\n",
    "* CNN1 = CNN(input, stride=1, kernal=1, depth=final_depth)\n",
    "* CNN2 = RELU(CNN(input, stride=1, kernal=1, depth=16))\n",
    "* CNN3 = RELU(CNN(input, stride=1, kernal=1, depth=16))\n",
    "* CNN4 = CNN(CNN2, string=1, kernal=3, depth=final_depth)\n",
    "* CNN5 = CNN(CNN3, stride=1, kernal=5, depth=final_depth)\n",
    "* POOL = MaxPool(input, stride=1, kernal=3)\n",
    "* CNN6 = CNN(Pool, stride=1, keranl=1, depth=final_depth)\n",
    "* => RELU(CONCAT(CNN1, CNN4, CNN5, CNN6)\n",
    "\n",
    "In the LeCunn paper, the author mentions the results for the model got much better after the results from the first 3 layers were combined and flatened before running the fully connected layer. This sound like an eary version of an inception module to me so I decided to take the time to learn how an inception module worked and use it in this project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "  weights = tf.get_variable('weights', \n",
    "    shape = [input.get_shape()[1], size],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "  )\n",
    "  biases = tf.get_variable('biases',\n",
    "    shape = [size],\n",
    "    initializer=tf.constant_initializer(0.0)\n",
    "  )\n",
    "  return tf.matmul(input, weights) + biases\n",
    "\n",
    "  \n",
    "def fully_connected_relu(input, size):\n",
    "  return tf.nn.relu(fully_connected(input, size))\n",
    "  \n",
    "def conv(input, kernel_size, depth):\n",
    "  weights = tf.get_variable('weights', \n",
    "    shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "  )\n",
    "  biases = tf.get_variable('biases',\n",
    "    shape = [depth],\n",
    "    initializer=tf.constant_initializer(0.0)\n",
    "  )\n",
    "  return tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME') + biases\n",
    "\n",
    "def conv_relu(input, kernel_size, depth):\n",
    "  return tf.nn.relu(conv(input, kernel_size, depth))\n",
    "\n",
    "def pool(input, filter_size, stride_size, padding='SAME'):\n",
    "  return tf.nn.max_pool(\n",
    "    input, \n",
    "    ksize=[1, filter_size, filter_size, 1], \n",
    "    strides=[1, stride_size, stride_size, 1], \n",
    "    padding=padding\n",
    "  )\n",
    "  \n",
    "def avg_pool(input, filter_size, stride_size, padding='SAME'):\n",
    "  return tf.nn.avg_pool(\n",
    "    input, \n",
    "    ksize=[1, filter_size, filter_size, 1], \n",
    "    strides=[1, stride_size, stride_size, 1], \n",
    "    padding=padding\n",
    "  )\n",
    "\n",
    "def inception(input, params):\n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_1x1_1\"):\n",
    "    conv_1x1_1 = conv(input, 1, params.final)\n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_1x1_2\"):\n",
    "    conv_1x1_2 = conv_relu(input, 1, params.reduce)\n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_1x1_3\"):\n",
    "    conv_1x1_3 = conv_relu(input, 1, params.reduce)\n",
    "    \n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_3x3\"):\n",
    "    conv_3x3 = conv(conv_1x1_2, 3, params.final)\n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_5x5\"):\n",
    "    conv_5x5 = conv(conv_1x1_3, 5, params.final)\n",
    "    \n",
    "  with tf.variable_scope(params.name + \"_\" + \"pool\"):\n",
    "    pool_1 = pool(input, 3, 1)\n",
    "  with tf.variable_scope(params.name + \"_\" + \"conv_1x1_4\"):\n",
    "    conv_1x1_4 = conv(pool_1, 1, params.final)\n",
    "    \n",
    "  return tf.nn.relu(tf.concat(3,[conv_1x1_1, conv_3x3, conv_5x5, conv_1x1_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def model_pass(input, is_training, params):\n",
    "  # Convolutional layers\n",
    "  with tf.variable_scope('conv1'):\n",
    "    conv1 = conv_relu(input, params['conv1'].kernel_size, params['conv1'].depth)\n",
    "        \n",
    "  with tf.variable_scope('inception1'):\n",
    "    inception1 = inception(conv1, params['inception1'])\n",
    "    \n",
    "  with tf.variable_scope('inception2'):\n",
    "    inception2 = inception(inception1, params['inception2']) \n",
    "    inception2 = tf.cond(is_training, lambda: tf.nn.dropout(inception2, keep_prob=params['dropout1'].keep_prob), lambda: inception2)\n",
    "        \n",
    "  # Flatten convolutional layers output\n",
    "  fc0 = flatten(inception2)\n",
    "    \n",
    "  # Fully connected layers\n",
    "  with tf.variable_scope('fc1'):\n",
    "    fc1 = fully_connected_relu(fc0, size=params['fc1'].size)\n",
    "    fc1 = tf.cond(is_training, lambda: tf.nn.dropout(fc1, keep_prob=params['dropout2'].keep_prob), lambda: fc1)\n",
    "  with tf.variable_scope('fc2'):\n",
    "    fc2 = fully_connected_relu(fc1, size=params['fc2'].size)\n",
    "    fc2 = tf.cond(is_training, lambda: tf.nn.dropout(fc2, keep_prob=params['dropout3'].keep_prob), lambda: fc2)\n",
    "  with tf.variable_scope('out'):\n",
    "    size = params['out'].size\n",
    "    prediction = fully_connected(fc2, size=size)\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from reusable.batches import Batch\n",
    "import os\n",
    "\n",
    "class ModelParam:\n",
    "  def __init__(self):\n",
    "    print('ModelParam.__init__')\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    print('ModelParam.full_param_name')\n",
    "\n",
    "class ConvParam(ModelParam):\n",
    "  def __init__(self, name, kernel_size, depth):\n",
    "    self.name = name\n",
    "    self.kernel_size = kernel_size\n",
    "    self.depth = depth\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    return \"ck{}cd{}\".format(self.kernel_size, self.depth)\n",
    "  \n",
    "class InceptionParam(ModelParam):\n",
    "  def __init__(self, name, final, reduce):\n",
    "    self.name = name\n",
    "    self.final = final\n",
    "    self.reduce = reduce\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    return \"id{}ir{}\".format(self.final, self.reduce)\n",
    "  \n",
    "class PoolParam(ModelParam):\n",
    "  def __init__(self, name, size):\n",
    "    self.name = name\n",
    "    self.size = size\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    return \"ps{}\".format(self.size)\n",
    "  \n",
    "class DropoutParam(ModelParam):\n",
    "  def __init__(self, name, keep_prob):\n",
    "    self.name = name\n",
    "    self.keep_prob = keep_prob\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    return \"dp{}\".format(self.keep_prob)\n",
    "  \n",
    "class FullyConnectedParam(ModelParam):\n",
    "  def __init__(self, name, size):\n",
    "    self.name = name\n",
    "    self.size = size\n",
    "  def full_param_name(self):\n",
    "    return \"fcd{}\".format(self.size)\n",
    "  \n",
    "class BaseParam(ModelParam):\n",
    "  def __init__(self, max_epochs, learning_rate, batch_size, should_train, run_tests, load_saved):\n",
    "    self.max_epochs = max_epochs\n",
    "    self.learning_rate = learning_rate\n",
    "    self.batch_size = batch_size\n",
    "    self.should_train = should_train\n",
    "    self.run_tests = run_tests\n",
    "    self.load_saved = load_saved\n",
    "    \n",
    "  def full_param_name(self):\n",
    "    return \"traffic_sign_classifier\"\n",
    "  \n",
    "class Params():\n",
    "  def __init__(self, params_dict, param_order):\n",
    "    self.params = params_dict\n",
    "    self.model_name = self._model_variable_scope(params_dict, param_order)\n",
    "    self.saved_path = self._file_path()\n",
    "    \n",
    "  def _model_variable_scope(self, params_dict, param_order):\n",
    "    model_variable_scope = \"_\".join([params_dict[k].full_param_name() for k in param_order]).replace(\".\", \"-\")\n",
    "    print(\"Model name:\", model_variable_scope)\n",
    "    return model_variable_scope\n",
    "  \n",
    "  def _file_path(self):\n",
    "    return os.getcwd() + \"/\" + self.model_name + \"-model\"\n",
    "  \n",
    "  def restore_session(self, saver, session):\n",
    "    try:\n",
    "      saver.restore(session, self.saved_path)\n",
    "      print(\"Restored session!\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed restoring previously trained model: file does not exist.\")\n",
    "        print(e)\n",
    "        pass\n",
    "  \n",
    "class EarlyStopping(): \n",
    "    def __init__(self, saver, session, patience = 30):\n",
    "        self.patience = patience\n",
    "        self.saver = saver\n",
    "        self.session = session\n",
    "        self.best_monitored_value = np.inf\n",
    "        self.best_monitored_epoch = 0\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, value, epoch):\n",
    "        if value < self.best_monitored_value:\n",
    "            self.best_monitored_value = value\n",
    "            self.best_monitored_epoch = epoch\n",
    "            self.restore_path = self.saver.save(self.session, os.getcwd() + \"/early_stopping_checkpoint\")\n",
    "        elif self.best_monitored_epoch + self.patience < epoch:\n",
    "            if self.restore_path != None:\n",
    "                self.saver.restore(self.session, self.restore_path)\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "def train_model(params, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "  model_shape = X_train[0].shape\n",
    "  num_keypoints = len(set(y_test))\n",
    "  graph = tf.Graph()\n",
    "  start = time.time()\n",
    "  \n",
    "  with graph.as_default():\n",
    "    tf_x_batch = tf.placeholder(tf.float32, shape = (None, model_shape[0],  model_shape[1], model_shape[2]))\n",
    "    tf_y_batch = tf.placeholder(tf.int32, shape = (None))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    one_hot_y_batch = tf.one_hot(tf_y_batch, num_keypoints)\n",
    "    current_epoch = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(params.params['base'].learning_rate, \n",
    "                                             current_epoch, \n",
    "                                             decay_steps = params.params['base'].max_epochs, \n",
    "                                             decay_rate = 0.01)\n",
    "    \n",
    "    # Training computation.\n",
    "    with tf.variable_scope(params.model_name):\n",
    "      logits = model_pass(tf_x_batch, is_training, params.params)\n",
    "\n",
    "    # This is what is run when training\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y_batch)\n",
    "    loss_operation = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    training_operation = optimizer.minimize(loss_operation)\n",
    "      \n",
    "    # This is what is run to evaluate the model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y_batch, 1))\n",
    "    accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "  with tf.Session(graph = graph) as session:\n",
    "    # Initialise all variables in the graph\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    early_stopping = EarlyStopping(tf.train.Saver(), session)\n",
    "    \n",
    "    def evaluate(X_data, y_data, batch_size):\n",
    "      batch = Batch(batch_size, X_data, y_data)\n",
    "      total_accuracy = 0\n",
    "      total_loss = 0\n",
    "      for batch_x, batch_y in batch.iterator():\n",
    "          [accuracy, loss] = session.run([accuracy_operation, loss_operation], feed_dict={tf_x_batch: batch_x, tf_y_batch: batch_y, is_training: False})\n",
    "          total_accuracy += (accuracy * len(batch_x))\n",
    "          total_loss += (loss * len(batch_x))\n",
    "      return (total_accuracy / len(X_data), total_loss / len(X_data))\n",
    "\n",
    "    def evaluate_precision_recall(X_data, y_data, batch_size):\n",
    "      y_pred = []\n",
    "      batch = Batch(batch_size, X_data, y_data)\n",
    "      for batch_x, batch_y in batch.iterator():\n",
    "        batch_y_pred = session.run(tf.argmax(logits, 1), feed_dict={tf_x_batch: batch_x, tf_y_batch: batch_y, is_training: False})\n",
    "        y_pred.append(batch_y_pred)\n",
    "\n",
    "      y_pred = np.concatenate(y_pred)    \n",
    "      precision, recall, f1_score, label_count = precision_recall_fscore_support(y_data, y_pred)\n",
    "\n",
    "      plt.plot(range(len(precision)), precision, marker='o', color='r')\n",
    "      plt.plot(range(len(precision)), recall, marker='o', color='g')\n",
    "      plt.plot(range(len(precision)), f1_score, marker='o', color='b')\n",
    "      plt.show()\n",
    "    \n",
    "    if params.params['base'].load_saved:\n",
    "      params.restore_session(saver, session)\n",
    "    \n",
    "    if params.params['base'].should_train:\n",
    "      print(\"Training...\")\n",
    "      print()\n",
    "      for epoch in range(params.params['base'].max_epochs):\n",
    "          current_epoch = epoch\n",
    "          X_train, y_train = shuffle(X_train, y_train)\n",
    "          batch = Batch(params.params['base'].batch_size, X_train, y_train)\n",
    "          for batch_x, batch_y in batch.iterator(): \n",
    "              session.run(training_operation, feed_dict={\n",
    "                tf_x_batch: batch_x, \n",
    "                tf_y_batch: batch_y, \n",
    "                is_training: True\n",
    "              })\n",
    "\n",
    "          valid_accuracy, valid_loss = evaluate(X_valid, y_valid, params.params['base'].batch_size)\n",
    "          train_accuracy, train_loss = evaluate(X_train, y_train, params.params['base'].batch_size)\n",
    "          print(\"EPOCH {} ...\".format(epoch+1))\n",
    "          print(\"Validation Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "          print(\"Validation Loss = {:.3f}\".format(valid_loss))\n",
    "          print()\n",
    "          print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "          print(\"Train Loss = {:.3f}\".format(train_loss))\n",
    "          print()\n",
    "\n",
    "\n",
    "          if early_stopping(valid_loss, epoch): \n",
    "              print(\"Early stopping.\\nBest monitored loss was {:.8f} at epoch {}.\".format(\n",
    "                  early_stopping.best_monitored_value, early_stopping.best_monitored_epoch\n",
    "              ))\n",
    "              break\n",
    "    \n",
    "    evaluate_precision_recall(X_valid, y_valid, params.params['base'].batch_size)\n",
    "    valid_accuracy, valid_loss = evaluate(X_valid, y_valid, params.params['base'].batch_size) \n",
    "    train_accuracy, train_loss = evaluate(X_train, y_train, params.params['base'].batch_size) \n",
    "    print(\"=============================================\")\n",
    "    print(\" Train loss: %.8f, accuracy = %.2f%%)\" % (train_loss, train_accuracy)) \n",
    "    print(\" Valid loss: %.8f, accuracy = %.2f%%)\" % (valid_loss, valid_accuracy)) \n",
    "    if params.params['base'].run_tests:\n",
    "      test_accuracy, test_loss = evaluate(X_test, y_test, params.params['base'].batch_size)\n",
    "      print(\" Test loss: %.8f, accuracy = %.2f%%)\" % (test_loss, test_accuracy))\n",
    "    else:\n",
    "        print(\"Not running model on test data right now.\") \n",
    "    print(\" Total time: {}\".format(start - time.time()))\n",
    "    print(\"  Timestamp: {}\".format(time.time()))\n",
    "        \n",
    "    saved_path = saver.save(session, params.saved_path)\n",
    "    print(\"Model saved to\", saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ck1cd1_id1ir1_id1ir1_dp0-8_fcd1_dp0-6_fcd1_dp0-5_fcd43_traffic_sign_classifier\n",
      "Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_params = {\n",
    "  'conv1': ConvParam(\"conv1\", 1, 1),\n",
    "  'inception1': InceptionParam(\"inception1\", 1, 1),\n",
    "  'inception2': InceptionParam(\"inception2\", 1, 1),\n",
    "  'dropout1': DropoutParam('dropout1', 0.8),\n",
    "  'fc1': FullyConnectedParam('fc1', 1),\n",
    "  'dropout2': DropoutParam('dropout2', 0.6),\n",
    "  'fc2': FullyConnectedParam('fc2', 1),\n",
    "  'dropout3': DropoutParam('dropout3', 0.5),\n",
    "  'out': FullyConnectedParam('out', 43),\n",
    "  'base': BaseParam(max_epochs=1, learning_rate=0.01, batch_size=1, \n",
    "                    should_train=True, run_tests=False, load_saved=False)\n",
    "}\n",
    "\n",
    "aws_params = {\n",
    "  'conv1': ConvParam(\"conv1\", 5, 16),\n",
    "  'inception1': InceptionParam(\"inception1\", 32, 16),\n",
    "  'inception2': InceptionParam(\"inception2\", 64, 16),\n",
    "  'dropout1': DropoutParam('dropout1', 0.8),\n",
    "  'fc1': FullyConnectedParam('fc1', 64),\n",
    "  'dropout2': DropoutParam('dropout2', 0.6),\n",
    "  'fc2': FullyConnectedParam('fc2', 128),\n",
    "  'dropout3': DropoutParam('dropout3', 0.5),\n",
    "  'out': FullyConnectedParam('out', 43),\n",
    "  'base': BaseParam(max_epochs=2, learning_rate=0.001, batch_size=16, \n",
    "                    should_train=True, run_tests=False, load_saved=False)\n",
    "}\n",
    "\n",
    "param_order = ['conv1', 'inception1', 'inception2', 'dropout1', 'fc1', 'dropout2', 'fc2', 'dropout3', 'out', 'base']\n",
    "\n",
    "if AWS:\n",
    "  params = Params(aws_params, param_order)\n",
    "  train_model(params, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "else:\n",
    "  params = Params(local_params, param_order)\n",
    "  train_model(params, X_train[0:100], y_train[0:100], X_valid[0:100], y_valid[0:100], X_test[0:100], y_test[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "I used the AdamOptamizer for this project. It seemed to work fine, although I did not spend the time to test a different optamizer. My batch size was very small, 16. I am not sure if I just had too many model paramameters or too large of a model but I struggled to fit my model in the AWS p2 GPU with anything larger (32, 64 ect). I tested different epoch sizes and felt like a max of 100 was a good size, but also allowed the model to stop early once the validation accuracy started to decrease within a threshold. I used the xavier_initializer to initialize the weights and set the initial biases to 0. My inital learning rate was 0.001 but I used a the tensorflow decay function so that learning rate would get smaller as the model progressed.\n",
    "\n",
    "Other hyperparameters include the depths of different layers, the size of some kernals in my CNNs, and my dropout rate between the two fully connected layers. I stared with dropouts around 0.5 but prograssivly moved it up to 0.7 as I seemed to get better results this way. I chose a first kernal size of 5 because that is what the paper did. From there I think the inception module provides an easy way to guarentee I get good results for different kernal sizes. \n",
    "\n",
    "I liked working with the inception module because it created a way for me to have less hypter paraemters to deal with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "I definetly user a process of trial and error for this project. I stated with the LeNet5 architicture as a baseline. This was iteration 0. This actually had a very good success rate around 95% on the validation set. I could have continued to tweek the depth parameters of the model and maybe gotten a better score. \n",
    "\n",
    "I then read the LeCunn paper and tried to reimpliment thier model. This was a pretty good model. I initally wanted to get rid of the max pooling and add dropout. I wanted to get rid of the max pooling because seems to be falling out of favor. I also wanted to add dropout because this is a good way to prevent over fitting. For this iteration fo the model, I ended up needing to add pooling to reduce my model size. The last part of the LeCunn paper talked about using the results from the first 3 CNN layers in the fuly connected layer. The paper also said this had a significant positive effect on the final result. To me this sounded like an early days inception module so I decided to create a new version with inception modules. \n",
    "\n",
    "After reading this [article](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/) and looking at the GooLeNet architechture, I decided to replace my middle two CNNs with inception modules. This felt right beause it still used a model about the size of the LeCunn moel but added a more sophisticated inception module. More trial and error is need to decide if that is the right solution. \n",
    "\n",
    "From there, I continued to tweak the model hyperparameters through trial and error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from reusable import web_images_loader \n",
    "\n",
    "DISPLAY = False\n",
    "\n",
    "def plot_web_images(web_images, web_image_labels, grey_web_images):\n",
    "  for i in range(len(web_images)):\n",
    "    print(\"=======================\")\n",
    "    print(\"Web image \", web_image_labels[i], \":\", sign_dict[str(web_image_labels[i])])\n",
    "    fig = plt.figure(figsize = (2, 1))\n",
    "    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "    axis1 = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "    axis2 = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "    axis1.imshow(web_images[i].squeeze())\n",
    "    axis2.imshow(grey_web_images[i].squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "if DISPLAY:\n",
    "  web_images = web_images_loader.load()\n",
    "  web_image_labels = web_images.labels\n",
    "  web_images = web_images.features\n",
    "  grey_web_images = preprocessor.to_greyscale(web_images)\n",
    "  X_web_image_data, y_web_image_data = preprocessor.preprocess(web_images, web_image_labels)\n",
    "  plot_web_images(web_images, web_image_labels, grey_web_images)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The images I found are mostly very high quality and have great contrast to thier backgrounds. There are a few that have some parts of the sign image covered or are not an exact match to the images provided in the project. These could be good ot test on. \n",
    "- End of speed limit (80km/h) [6]\n",
    "- Road work [25]\n",
    "- Beware of ice/snow [30]\n",
    "- Turn right ahead [33]\n",
    "- Turn left ahead [34]\n",
    "- Keep right [38]\n",
    "- End of no passing [42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def get_predictions(params, features, labels):\n",
    "    graph = tf.Graph()\n",
    "    model_shape = X_train[0].shape\n",
    "    \n",
    "    with graph.as_default():\n",
    "      tf_x_batch = tf.placeholder(tf.float32, shape = (None, model_shape[0],  model_shape[1], model_shape[2]))\n",
    "      is_training = tf.placeholder(tf.bool)\n",
    "      \n",
    "      with tf.variable_scope(params.model_name):\n",
    "        predictions = tf.nn.softmax(model_pass(tf_x_batch, is_training, params.params))\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        params.restore_session(tf.train.Saver(), session)\n",
    "        batch = Batch(params.params['base'].batch_size, features, labels)\n",
    "        all_predictions = []\n",
    "        for batch_x, batch_y in batch.iterator():  \n",
    "          batch_predictions = session.run(predictions, feed_dict = {tf_x_batch: batch_x, is_training: False})\n",
    "          all_predictions.append(batch_predictions)\n",
    "\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "        top_5 = session.run(tf.nn.top_k(all_predictions, 5))\n",
    "        return top_5, all_predictions\n",
    "     \n",
    "def evaluate_web_images(start=0, stop=43):\n",
    "  web_image_params = params.params\n",
    "  web_image_params['base'] = BaseParam(max_epochs=0, learning_rate=0.001, batch_size=16, \n",
    "                      should_train=False, run_tests=False, load_saved=True)\n",
    "  web_params = Params(web_image_params, param_order)\n",
    "\n",
    "  top_5, predictions = get_predictions(params, X_web_image_data[start:stop], y_web_image_data[start:stop])\n",
    "  \n",
    "  for i in range(start, stop):\n",
    "    index = web_image_labels.index(y_web_image_data[i])\n",
    "    plot_web_image_prediction(web_images[index], grey_web_images[index], y_web_image_data[i], top_5[0][i-start], top_5[1][i-start])\n",
    "\n",
    "  onehot_labels = LabelBinarizer().fit_transform(y_web_image_data)\n",
    "  correct_prediction = np.equal(np.argmax(predictions, 1), np.argmax(onehot_labels[start:stop], 1))\n",
    "  accuracy = np.mean(correct_prediction)\n",
    "  \n",
    "  print(\"Web image accuracy = {:.3f}%\".format(accuracy * 100.0))\n",
    "  \n",
    "def plot_web_image_prediction(orig, grey_image, value, pred_percent, pred_labels):\n",
    "  # Prepare the grid\n",
    "  plt.figure(figsize = (6, 2))\n",
    "  gridspec.GridSpec(2, 2)\n",
    "\n",
    "  # Plot original image\n",
    "  plt.subplot2grid((2, 2), (0, 0), colspan=1, rowspan=1)\n",
    "  plt.imshow(orig.squeeze())\n",
    "  plt.axis('off')\n",
    "\n",
    "  # Plot preprocessed image\n",
    "  plt.subplot2grid((2, 2), (1, 0), colspan=1, rowspan=1)\n",
    "  plt.imshow(grey_image.squeeze(), cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "  # Plot predictions\n",
    "  plt.subplot2grid((2, 2), (0, 1), colspan=1, rowspan=2)\n",
    "  plt.barh(np.arange(5)+.5, pred_percent, align='center')\n",
    "  plt.yticks(np.arange(5)+.5, [sign_dict[str(label)] for label in pred_labels])\n",
    "  plt.tick_params(axis='both', which='both', labelleft='off', labelright='on', labeltop='off', labelbottom='off')\n",
    "\n",
    "  plt.show()\n",
    "  \n",
    "if DISPLAY:\n",
    "  evaluate_web_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
